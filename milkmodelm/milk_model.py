# -*- coding: utf-8 -*-
"""milk_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yUxy__zcIh4MXE39V702vM8P02V7YzHq

MILK MODEL
"""

#importing dependencies
import tarfile
import urllib
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.preprocessing import StandardScaler

#loading the dataset
training_data = pd.read_csv('/content/trainingdata.csv')

training_data.head()

training_data.shape

#Checking for null values 
training_data[training_data.isnull().any(axis=1)].count()

#removing null values
training_data = training_data.dropna()
training_data.isnull().sum()

np.shape(training_data)

# check for missing values
missing_values = training_data.isnull().sum().sort_values(ascending = False)
missing_values = missing_values[missing_values > 0]/training_data.shape[0] # normalize
print(f'{missing_values *100} %')

training_data.dropna(inplace= True)

df = pd.read_csv ('/content/trainingdata.csv')
print(df)

for col in df:
  print(df[col].unique())

#removing null values
df = df.dropna()
df.isnull().sum()

# check for missing values
missing_values = df.isnull().sum().sort_values(ascending = False)
missing_values = missing_values[missing_values > 0]/df.shape[0] # normalize
print(f'{missing_values *100} %')

df.dropna(inplace= True)

for col in df:
  print(df[col].unique())

'''The high EC for healthy =5.48mS/cm,
 subclinical mastitis milk = 6.3mS/cm
 clinical mastitis milk = 8.5mS/cm '''

df.columns

df = df[df["Conductivity.LF"].str.contains("#NAME?") == False]
df = df[df["Conductivity.LR"].str.contains("#NAME?") == False]
df = df[df["Conductivity.RF"].str.contains("#NAME?") == False]
df = df[df["Conductivity.RR"].str.contains("#NAME?") == False]

"""Dropping the necessary columns for KMEANS clustering"""

data = df.drop(['animal.num', 'Consumed', 'Yield', 'Counsumed.times', 'Total.Duration',
       'DaysInMilk','lactation', 'month', 'age'], axis=1)
data.head()

#droping null values
data=data.dropna()

#create scaled DataFrame where each variable has mean of 0 and standard dev of 1
scaled_df = StandardScaler().fit_transform(data)

#view first five rows of scaled DataFrame
print(scaled_df[:5])

""" KMEANS ALGORITHM
 Performing a k-means clustering 
"""

#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": 1,
}

#create list to hold SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_df)
    sse.append(kmeans.inertia_)

#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

"""Typically when we create this type of plot we look for an “elbow” where the sum of squares begins to “bend” or level off. This is typically the optimal number of clusters. In this plot it appears that there is an elbow or “bend” at k = 3 clusters.

Perform K-Means Clustering with Optimal K
"""

#instantiate the k-means class, using optimal number of clusters
kmeans = KMeans(init="random", n_clusters=3, n_init=10, random_state=1)

#fit k-means algorithm to data
kmeans.fit(scaled_df)

#view cluster assignments for each observation
kmeans.labels_

""" To interpret, we can add a column to the DataFrame that shows the cluster assignment of each Conductivity:"""

#append cluster assingments to original DataFrame
data['cluster'] = kmeans.labels_

#view updated DataFrame
print(data)

"""'''The high EC for healthy =5.48mS/cm,
 subclinical mastitis milk = 6.3mS/cm
 clinical mastitis milk = 8.5mS/cm '''

VISUALIZING THE DATA
"""

#Initialize the class object
kmeans = KMeans(n_clusters= 3)
 
#predict the labels of clusters.
label_pred= kmeans.fit_predict(data)

"""Plotting the KMeans Clusters"""



"""MODEL ACCURACY"""

#Evaluating the performance by calculating the silhouette coefficient:
silhouette_score(data,label_pred)

#Calculate Adjusted Rand Index, too, since the ground truth cluster labels are available:
adjusted_rand_score(kmeans.labels_, label_pred)

#accuracy score

"""NOTE: The scale for each of these clustering performance metrics ranges from -1 to 1. A silhouette coefficient of 0 indicates that clusters are significantly overlapping one another, and a silhouette coefficient of 1 indicates clusters are well-separated. An ARI score of 0 indicates that cluster labels are randomly assigned, and an ARI score of 1 means that the true labels and predicted labels form identical clusters.

# BUILDING A CLASSIFICATION MODEL

DECISION TREE
"""

#creating a dataframe
new_data = pd.DataFrame(data)
'''#spliting the data into target and features
X = data.drop(columns = 'target' )
y = data.target'''
new_data.head()

# Renaming some of the columns 
new_data = new_data.rename(columns={'cluster':'target'})

#spliting the data into target and features
X = data.iloc[:,:4].values
y = data.iloc[:,4].values
print(X)

#spliting the dataset into training and test.
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test =  train_test_split(X,y,test_size = 0.25, random_state= 0)

#Performing feature scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# Fitting the model in the Decision Tree classifier
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier = classifier.fit(X_train,y_train)

"""MODEL ACCURACY"""

#prediction
y_pred = classifier.predict(X_test)
print(y_pred)

#Accuracy
from sklearn import metrics 
print('Accuracy Score:', metrics.accuracy_score(y_test,y_pred))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

"""VISUALIZING THE PREDICTED MODEL

Optimizing the Decision Tree Classifier
"""

# Create Decision Tree classifer object
classifier = DecisionTreeClassifier(criterion="entropy", max_depth=3)
# Train Decision Tree Classifer
classifier = classifier.fit(X_train,y_train)
#Predict the response for test dataset
y_pred = classifier.predict(X_test)
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

"""BUILDING A PREDICTIVE SYSTEM"""

input_data = (5.43,4.99,5.32,4.53)
#converting the input data into a numpyarray 
input_conductivity = np.asarray(input_data)

#reshaping the numpy array
input_cond_reshaped = input_conductivity.reshape(1,-1)
input_cond_reshaped.shape

#standardizing the data 
input_cond_std = sc_X.transform(input_cond_reshaped)

'''#create scaled DataFrame where each variable has mean of 0 and standard dev of 1
input_conductivity_std= scaled_df.transform(input_cond_reshaped)'''

prediction= classifier.predict(input_cond_std)

print(prediction)



if(prediction[0]==2):
    print("Healthy animal and no mastitis Detected")
  
elif (prediction[0]==1):
    print ("clinical mastitis Detected")

else:
    print("Subclinical mastitis Detected")

"""SAVING THE MODEL"""

import pickle
filename = 'milk_model.pkl'
pickle.dump(classifier,open(filename,'wb'))





